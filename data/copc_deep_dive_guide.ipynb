{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COPC Deep Dive: Internals, Optimization & Visualization\n",
    "\n",
    "A comprehensive technical guide to Cloud-Optimized Point Cloud (COPC) format.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **COPC Format Internals** - File structure, VLRs, hierarchy\n",
    "2. **Octree Organization** - Node keys, spatial indexing, LODs\n",
    "3. **Data Access** - Byte offsets, chunk reading, HTTP ranges\n",
    "4. **Point Decoding** - Scales, offsets, extra dimensions\n",
    "5. **Visualization Optimization** - Progressive loading, culling, rendering\n",
    "6. **Common Issues** - Memory, latency, browser limits\n",
    "7. **Practical Examples** - Real code with CALIPSO data\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of point clouds and spatial data\n",
    "- Basic knowledge of binary file formats\n",
    "- Familiarity with octrees (helpful but not required)\n",
    "\n",
    "## References\n",
    "\n",
    "- [COPC Specification](https://copc.io/)\n",
    "- [LAS 1.4 Specification](https://www.asprs.org/divisions-committees/lidar-division/laser-las-file-format-exchange-activities)\n",
    "- [LAZ Compression](https://laszip.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: COPC Format Overview\n",
    "\n",
    "### What is COPC?\n",
    "\n",
    "**COPC (Cloud-Optimized Point Cloud)** is an extension of the LAS 1.4 format that organizes point data in an **octree structure** to enable efficient streaming and partial reads over HTTP.\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "| Feature | Traditional LAS/LAZ | COPC |\n",
    "|---------|---------------------|------|\n",
    "| **Organization** | Sequential points | Octree-organized chunks |\n",
    "| **Streaming** | Must download entire file | Read specific regions |\n",
    "| **LOD Support** | No | Built-in multi-resolution |\n",
    "| **HTTP Range** | Not practical | Designed for it |\n",
    "| **File Size** | Baseline | Similar with LAZ compression |\n",
    "\n",
    "### Why COPC for CALIPSO Data?\n",
    "\n",
    "1. **Large datasets** - CALIPSO files can be 300+ MB after conversion\n",
    "2. **Sparse 3D structure** - Satellite lidar creates vertical curtains\n",
    "3. **Progressive loading** - Show low-res data quickly, refine on demand\n",
    "4. **Web visualization** - Stream from cloud storage without full download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import laspy\n",
    "import struct\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# COPC file to analyze\n",
    "COPC_FILE = Path('final/CAL_LID_L1-Standard-V4-51.2023-06-30T16-44-43ZD.copc.laz')\n",
    "\n",
    "if not COPC_FILE.exists():\n",
    "    print(f\"âŒ COPC file not found: {COPC_FILE}\")\n",
    "    print(\"   Run the conversion notebook first!\")\n",
    "else:\n",
    "    file_size_mb = COPC_FILE.stat().st_size / (1024**2)\n",
    "    print(f\"âœ… Using COPC file: {COPC_FILE.name}\")\n",
    "    print(f\"   File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: COPC File Structure\n",
    "\n",
    "A COPC file is a valid LAS 1.4 file with additional **Variable Length Records (VLRs)** that define the octree structure.\n",
    "\n",
    "### File Layout\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LAS Header (375 bytes)             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  VLRs (Variable Length Records)     â”‚\n",
    "â”‚  â”œâ”€ CRS/Projection                  â”‚\n",
    "â”‚  â”œâ”€ Extra Bytes (dimensions)        â”‚\n",
    "â”‚  â””â”€ COPC Info VLR â­                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Point Data (organized as chunks)   â”‚\n",
    "â”‚  â”œâ”€ Root node (0-0-0-0)             â”‚\n",
    "â”‚  â”œâ”€ Child nodes (1-0-0-0, ...)      â”‚\n",
    "â”‚  â””â”€ ... recursive octree ...        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  EVLRs (Extended VLRs)              â”‚\n",
    "â”‚  â””â”€ COPC Hierarchy VLR â­â­         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### COPC Info VLR\n",
    "\n",
    "Located in the header VLRs, contains:\n",
    "- **Center**: [x, y, z] octree center point\n",
    "- **Halfsize**: Size of root node bounding cube\n",
    "- **Spacing**: Minimum point spacing at deepest level\n",
    "- **Root hierarchy offset/size**: Where to find the hierarchy\n",
    "\n",
    "### COPC Hierarchy VLR\n",
    "\n",
    "Located at the end of the file (EVLR), contains:\n",
    "- Array of **Entry** structures (32 bytes each)\n",
    "- Each entry describes one octree node\n",
    "- Mapping from VoxelKey â†’ byte offset/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Read LAS header and basic info\nlas = laspy.read(COPC_FILE)\n\nprint(\"=\"*80)\nprint(\"LAS HEADER INFORMATION\")\nprint(\"=\"*80)\nprint(f\"\\nVersion: {las.header.version}\")\nprint(f\"Point format: {las.header.point_format}\")\nprint(f\"Point count: {len(las.points):,}\")\nprint(f\"\\nBounds:\")\nprint(f\"  X: [{las.header.x_min:.6f}, {las.header.x_max:.6f}]\")\nprint(f\"  Y: [{las.header.y_min:.6f}, {las.header.y_max:.6f}]\")\nprint(f\"  Z: [{las.header.z_min:.6f}, {las.header.z_max:.6f}]\")\nprint(f\"\\nScales:\")\nprint(f\"  X: {las.header.scales[0]}\")\nprint(f\"  Y: {las.header.scales[1]}\")\nprint(f\"  Z: {las.header.scales[2]}\")\nprint(f\"\\nOffsets:\")\nprint(f\"  X: {las.header.offsets[0]}\")\nprint(f\"  Y: {las.header.offsets[1]}\")\nprint(f\"  Z: {las.header.offsets[2]}\")\n\nprint(f\"\\nPoint Format:\")\nprint(f\"  Point size: {las.header.point_format.size} bytes per point\")\nprint(f\"  Point format ID: {las.header.point_format.id}\")\n\nprint(f\"\\nVLRs: {len(las.header.vlrs)}\")\nfor i, vlr in enumerate(las.header.vlrs):\n    print(f\"  [{i}] {vlr.user_id}: {vlr.description}\")\n\nprint(f\"\\nExtra Dimensions: {list(las.point_format.extra_dimension_names)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and parse COPC Info VLR\n",
    "# Note: laspy may not expose COPC-specific VLRs directly, so we'll read the raw file\n",
    "\n",
    "def find_copc_info_vlr(filename):\n",
    "    \"\"\"\n",
    "    Find and parse the COPC Info VLR from a COPC file.\n",
    "    \n",
    "    COPC Info VLR structure (160 bytes):\n",
    "    - center_x, center_y, center_z: float64 (8 bytes each)\n",
    "    - halfsize: float64\n",
    "    - spacing: float64\n",
    "    - root_hier_offset: uint64\n",
    "    - root_hier_size: uint64\n",
    "    - gpstime_minimum: float64\n",
    "    - gpstime_maximum: float64\n",
    "    - reserved: 64 bytes\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read LAS header\n",
    "        f.seek(0)\n",
    "        signature = f.read(4)\n",
    "        if signature != b'LASF':\n",
    "            raise ValueError(\"Not a valid LAS file\")\n",
    "        \n",
    "        # Read header size and number of VLRs\n",
    "        f.seek(94)\n",
    "        header_size = struct.unpack('<H', f.read(2))[0]\n",
    "        \n",
    "        f.seek(100)\n",
    "        num_vlrs = struct.unpack('<I', f.read(4))[0]\n",
    "        \n",
    "        print(f\"Header size: {header_size} bytes\")\n",
    "        print(f\"Number of VLRs: {num_vlrs}\")\n",
    "        \n",
    "        # VLRs start after header\n",
    "        vlr_offset = header_size\n",
    "        \n",
    "        for i in range(num_vlrs):\n",
    "            f.seek(vlr_offset)\n",
    "            \n",
    "            # VLR header (54 bytes)\n",
    "            reserved = struct.unpack('<H', f.read(2))[0]\n",
    "            user_id = f.read(16).decode('utf-8').rstrip('\\x00')\n",
    "            record_id = struct.unpack('<H', f.read(2))[0]\n",
    "            record_length = struct.unpack('<H', f.read(2))[0]\n",
    "            description = f.read(32).decode('utf-8', errors='ignore').rstrip('\\x00')\n",
    "            \n",
    "            print(f\"\\nVLR {i}: {user_id} (ID: {record_id})\")\n",
    "            print(f\"  Description: {description}\")\n",
    "            print(f\"  Length: {record_length} bytes\")\n",
    "            \n",
    "            # Check for COPC Info VLR\n",
    "            if user_id == 'copc' and record_id == 1:\n",
    "                print(\"  â­ COPC Info VLR found!\")\n",
    "                \n",
    "                # Parse COPC Info structure\n",
    "                center_x, center_y, center_z = struct.unpack('<ddd', f.read(24))\n",
    "                halfsize = struct.unpack('<d', f.read(8))[0]\n",
    "                spacing = struct.unpack('<d', f.read(8))[0]\n",
    "                root_hier_offset = struct.unpack('<Q', f.read(8))[0]\n",
    "                root_hier_size = struct.unpack('<Q', f.read(8))[0]\n",
    "                gpstime_min = struct.unpack('<d', f.read(8))[0]\n",
    "                gpstime_max = struct.unpack('<d', f.read(8))[0]\n",
    "                \n",
    "                copc_info = {\n",
    "                    'center': [center_x, center_y, center_z],\n",
    "                    'halfsize': halfsize,\n",
    "                    'spacing': spacing,\n",
    "                    'root_hier_offset': root_hier_offset,\n",
    "                    'root_hier_size': root_hier_size,\n",
    "                    'gpstime_range': [gpstime_min, gpstime_max]\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n  Center: ({center_x:.6f}, {center_y:.6f}, {center_z:.6f})\")\n",
    "                print(f\"  Halfsize: {halfsize:.6f}\")\n",
    "                print(f\"  Spacing: {spacing:.6f}\")\n",
    "                print(f\"  Hierarchy offset: {root_hier_offset:,} bytes\")\n",
    "                print(f\"  Hierarchy size: {root_hier_size:,} bytes\")\n",
    "                print(f\"  GPS time: {gpstime_min:.2f} to {gpstime_max:.2f}\")\n",
    "                \n",
    "                return copc_info\n",
    "            \n",
    "            # Move to next VLR\n",
    "            vlr_offset += 54 + record_length\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SEARCHING FOR COPC INFO VLR\")\n",
    "print(\"=\"*80)\n",
    "copc_info = find_copc_info_vlr(COPC_FILE)\n",
    "\n",
    "if copc_info:\n",
    "    print(\"\\nâœ… COPC file confirmed with valid Info VLR\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  COPC Info VLR not found - may not be a valid COPC file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Octree Nodes & Spatial Indexing\n",
    "\n",
    "### Octree Structure\n",
    "\n",
    "COPC organizes points in an **octree** - a tree where each node has 8 children (or fewer at boundaries).\n",
    "\n",
    "```\n",
    "                    Root (0-0-0-0)\n",
    "                    Level 0\n",
    "                        |\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚       â”‚       â”‚       â”‚       â”‚\n",
    "    1-0-0-0 1-1-0-0 1-2-0-0  ... 1-7-0-0\n",
    "    Level 1 (8 children)\n",
    "        â”‚\n",
    "    â”Œâ”€â”€â”€â”´â”€â”€â”€â”\n",
    "    â”‚       â”‚\n",
    "2-0-0-0  2-1-0-0 ...\n",
    "Level 2 (8 children each)\n",
    "```\n",
    "\n",
    "### VoxelKey Encoding\n",
    "\n",
    "Each node is identified by a **VoxelKey**: `(depth, x, y, z)`\n",
    "\n",
    "- **depth**: Distance from root (0 = root)\n",
    "- **x, y, z**: Morton-encoded spatial coordinates\n",
    "\n",
    "### Child Indexing\n",
    "\n",
    "For a node at (d, x, y, z), its 8 children are at depth d+1:\n",
    "\n",
    "```python\n",
    "children = [\n",
    "    (d+1, 2*x+0, 2*y+0, 2*z+0),  # Child 0: -x, -y, -z\n",
    "    (d+1, 2*x+1, 2*y+0, 2*z+0),  # Child 1: +x, -y, -z\n",
    "    (d+1, 2*x+0, 2*y+1, 2*z+0),  # Child 2: -x, +y, -z\n",
    "    (d+1, 2*x+1, 2*y+1, 2*z+0),  # Child 3: +x, +y, -z\n",
    "    (d+1, 2*x+0, 2*y+0, 2*z+1),  # Child 4: -x, -y, +z\n",
    "    (d+1, 2*x+1, 2*y+0, 2*z+1),  # Child 5: +x, -y, +z\n",
    "    (d+1, 2*x+0, 2*y+1, 2*z+1),  # Child 6: -x, +y, +z\n",
    "    (d+1, 2*x+1, 2*y+1, 2*z+1),  # Child 7: +x, +y, +z\n",
    "]\n",
    "```\n",
    "\n",
    "### Bounding Box Calculation\n",
    "\n",
    "Given a VoxelKey, calculate its 3D bounding box:\n",
    "\n",
    "```python\n",
    "def get_node_bounds(depth, x, y, z, center, halfsize):\n",
    "    # Size of this node\n",
    "    node_size = halfsize / (2 ** depth)\n",
    "    \n",
    "    # Position relative to center\n",
    "    min_x = center[0] - halfsize + x * 2 * node_size\n",
    "    min_y = center[1] - halfsize + y * 2 * node_size\n",
    "    min_z = center[2] - halfsize + z * 2 * node_size\n",
    "    \n",
    "    return [\n",
    "        [min_x, min_x + 2*node_size],\n",
    "        [min_y, min_y + 2*node_size],\n",
    "        [min_z, min_z + 2*node_size]\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VoxelKey utilities\n",
    "\n",
    "class VoxelKey:\n",
    "    \"\"\"Represents a node in the COPC octree.\"\"\"\n",
    "    \n",
    "    def __init__(self, depth, x, y, z):\n",
    "        self.depth = depth\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"VoxelKey({self.depth}-{self.x}-{self.y}-{self.z})\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return (self.depth == other.depth and \n",
    "                self.x == other.x and \n",
    "                self.y == other.y and \n",
    "                self.z == other.z)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.depth, self.x, self.y, self.z))\n",
    "    \n",
    "    def get_children(self):\n",
    "        \"\"\"Get all 8 child VoxelKeys.\"\"\"\n",
    "        d = self.depth + 1\n",
    "        return [\n",
    "            VoxelKey(d, 2*self.x+0, 2*self.y+0, 2*self.z+0),\n",
    "            VoxelKey(d, 2*self.x+1, 2*self.y+0, 2*self.z+0),\n",
    "            VoxelKey(d, 2*self.x+0, 2*self.y+1, 2*self.z+0),\n",
    "            VoxelKey(d, 2*self.x+1, 2*self.y+1, 2*self.z+0),\n",
    "            VoxelKey(d, 2*self.x+0, 2*self.y+0, 2*self.z+1),\n",
    "            VoxelKey(d, 2*self.x+1, 2*self.y+0, 2*self.z+1),\n",
    "            VoxelKey(d, 2*self.x+0, 2*self.y+1, 2*self.z+1),\n",
    "            VoxelKey(d, 2*self.x+1, 2*self.y+1, 2*self.z+1),\n",
    "        ]\n",
    "    \n",
    "    def get_bounds(self, center, halfsize):\n",
    "        \"\"\"Calculate 3D bounding box for this node.\"\"\"\n",
    "        node_size = halfsize / (2 ** self.depth)\n",
    "        \n",
    "        min_x = center[0] - halfsize + self.x * 2 * node_size\n",
    "        min_y = center[1] - halfsize + self.y * 2 * node_size\n",
    "        min_z = center[2] - halfsize + self.z * 2 * node_size\n",
    "        \n",
    "        return {\n",
    "            'min': [min_x, min_y, min_z],\n",
    "            'max': [min_x + 2*node_size, min_y + 2*node_size, min_z + 2*node_size],\n",
    "            'size': 2*node_size\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "root = VoxelKey(0, 0, 0, 0)\n",
    "print(f\"Root node: {root}\")\n",
    "print(f\"\\nRoot's 8 children:\")\n",
    "for i, child in enumerate(root.get_children()):\n",
    "    print(f\"  Child {i}: {child}\")\n",
    "\n",
    "if copc_info:\n",
    "    bounds = root.get_bounds(copc_info['center'], copc_info['halfsize'])\n",
    "    print(f\"\\nRoot node bounds:\")\n",
    "    print(f\"  Min: ({bounds['min'][0]:.2f}, {bounds['min'][1]:.2f}, {bounds['min'][2]:.2f})\")\n",
    "    print(f\"  Max: ({bounds['max'][0]:.2f}, {bounds['max'][1]:.2f}, {bounds['max'][2]:.2f})\")\n",
    "    print(f\"  Size: {bounds['size']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Byte Offsets & Chunk Reading\n",
    "\n",
    "### COPC Hierarchy Structure\n",
    "\n",
    "The hierarchy is an array of **Entry** structures (32 bytes each):\n",
    "\n",
    "```c\n",
    "struct Entry {\n",
    "    VoxelKey key;           // 16 bytes (int32 Ã— 4)\n",
    "    int64_t  offset;        // 8 bytes - byte offset to point data\n",
    "    int32_t  byteSize;      // 4 bytes - size of point data chunk\n",
    "    int32_t  pointCount;    // 4 bytes - number of points in chunk\n",
    "};\n",
    "```\n",
    "\n",
    "### Reading Specific Nodes\n",
    "\n",
    "To read points from a specific node:\n",
    "\n",
    "1. **Find the node** in the hierarchy\n",
    "2. **Get byte offset and size**\n",
    "3. **Seek to offset** in the file\n",
    "4. **Read byteSize bytes**\n",
    "5. **Decompress** if LAZ compressed\n",
    "6. **Parse point data**\n",
    "\n",
    "### HTTP Range Requests\n",
    "\n",
    "For cloud-hosted COPC files:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def read_node_http(url, offset, byte_size):\n",
    "    \"\"\"Read a specific node chunk via HTTP range request.\"\"\"\n",
    "    headers = {\n",
    "        'Range': f'bytes={offset}-{offset + byte_size - 1}'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.content\n",
    "```\n",
    "\n",
    "This allows **streaming** without downloading the entire file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse COPC Hierarchy\n",
    "\n",
    "def parse_copc_hierarchy(filename, hier_offset, hier_size):\n",
    "    \"\"\"\n",
    "    Parse the COPC hierarchy from the file.\n",
    "    Returns a dictionary mapping VoxelKey -> Entry info.\n",
    "    \"\"\"\n",
    "    hierarchy = {}\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        f.seek(hier_offset)\n",
    "        \n",
    "        # Each entry is 32 bytes\n",
    "        entry_size = 32\n",
    "        num_entries = hier_size // entry_size\n",
    "        \n",
    "        print(f\"Reading {num_entries:,} hierarchy entries...\")\n",
    "        \n",
    "        for i in range(num_entries):\n",
    "            # VoxelKey: 4 Ã— int32\n",
    "            depth, x, y, z = struct.unpack('<iiii', f.read(16))\n",
    "            \n",
    "            # Entry data\n",
    "            offset = struct.unpack('<q', f.read(8))[0]  # int64\n",
    "            byte_size = struct.unpack('<i', f.read(4))[0]  # int32\n",
    "            point_count = struct.unpack('<i', f.read(4))[0]  # int32\n",
    "            \n",
    "            key = VoxelKey(depth, x, y, z)\n",
    "            hierarchy[key] = {\n",
    "                'offset': offset,\n",
    "                'byte_size': byte_size,\n",
    "                'point_count': point_count\n",
    "            }\n",
    "            \n",
    "            if i < 10:  # Show first 10 entries\n",
    "                print(f\"  {key}: offset={offset:,}, size={byte_size:,}, points={point_count:,}\")\n",
    "        \n",
    "        if num_entries > 10:\n",
    "            print(f\"  ... and {num_entries - 10:,} more entries\")\n",
    "    \n",
    "    return hierarchy\n",
    "\n",
    "if copc_info:\n",
    "    print(\"=\"*80)\n",
    "    print(\"PARSING COPC HIERARCHY\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    hierarchy = parse_copc_hierarchy(\n",
    "        COPC_FILE,\n",
    "        copc_info['root_hier_offset'],\n",
    "        copc_info['root_hier_size']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Loaded {len(hierarchy):,} nodes from hierarchy\")\n",
    "    \n",
    "    # Analyze hierarchy\n",
    "    depths = [key.depth for key in hierarchy.keys()]\n",
    "    max_depth = max(depths)\n",
    "    \n",
    "    print(f\"\\nHierarchy statistics:\")\n",
    "    print(f\"  Maximum depth: {max_depth}\")\n",
    "    print(f\"  Nodes per depth:\")\n",
    "    for d in range(max_depth + 1):\n",
    "        count = sum(1 for depth in depths if depth == d)\n",
    "        total_points = sum(hierarchy[k]['point_count'] for k in hierarchy if k.depth == d)\n",
    "        print(f\"    Level {d}: {count:,} nodes, {total_points:,} points\")\nelse:\n",
    "    print(\"âš ï¸  Cannot parse hierarchy without COPC Info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Point Data Decoding\n",
    "\n",
    "### Coordinate Transformation\n",
    "\n",
    "Points are stored as **scaled integers** for compression:\n",
    "\n",
    "```python\n",
    "# Stored in file (int32)\n",
    "x_int, y_int, z_int = point_record\n",
    "\n",
    "# Real-world coordinates\n",
    "x_real = x_int * scale_x + offset_x\n",
    "y_real = y_int * scale_y + offset_y\n",
    "z_real = z_int * scale_z + offset_z\n",
    "```\n",
    "\n",
    "### Point Format 6 Structure\n",
    "\n",
    "Our CALIPSO COPC uses Point Format 6 (30 bytes base + 16 bytes extra):\n",
    "\n",
    "| Field | Type | Bytes | Description |\n",
    "|-------|------|-------|-------------|\n",
    "| X | int32 | 4 | Scaled X coordinate |\n",
    "| Y | int32 | 4 | Scaled Y coordinate |\n",
    "| Z | int32 | 4 | Scaled Z coordinate |\n",
    "| Intensity | uint16 | 2 | Backscatter intensity (scaled) |\n",
    "| Return info | uint8 | 1 | Return number, etc. |\n",
    "| Classification | uint8 | 1 | Point classification |\n",
    "| Scan angle | int16 | 2 | Scanner angle |\n",
    "| User data | uint8 | 1 | User-defined |\n",
    "| Point source | uint16 | 2 | Source ID |\n",
    "| GPS time | float64 | 8 | Timestamp |\n",
    "| **Extra bytes** | | 16 | |\n",
    "| backscatter_532 | float64 | 8 | 532nm backscatter |\n",
    "| backscatter_1064 | float64 | 8 | 1064nm backscatter |\n",
    "\n",
    "### Decoding Example\n",
    "\n",
    "```python\n",
    "# Read point from node\n",
    "point_bytes = chunk[i*46:(i+1)*46]  # 46 bytes per point\n",
    "\n",
    "# Parse coordinates\n",
    "x_int, y_int, z_int = struct.unpack('<iii', point_bytes[0:12])\n",
    "x = x_int * scale_x + offset_x\n",
    "y = y_int * scale_y + offset_y\n",
    "z = z_int * scale_z + offset_z\n",
    "\n",
    "# Parse intensity\n",
    "intensity = struct.unpack('<H', point_bytes[12:14])[0]\n",
    "\n",
    "# Parse extra dimensions\n",
    "bs_532 = struct.unpack('<d', point_bytes[30:38])[0]\n",
    "bs_1064 = struct.unpack('<d', point_bytes[38:46])[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate coordinate scaling\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COORDINATE SCALING EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get first 5 points\n",
    "sample_points = 5\n",
    "\n",
    "print(f\"\\nShowing first {sample_points} points:\\n\")\n",
    "print(f\"{'Index':<8} {'X (Â°)':<15} {'Y (Â°)':<15} {'Z (km)':<12} {'Intensity':<12} {'BS_532':<12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for i in range(min(sample_points, len(las.points))):\n",
    "    x = las.x[i]\n",
    "    y = las.y[i]\n",
    "    z = las.z[i]\n",
    "    intensity = las.intensity[i]\n",
    "    bs532 = las.backscatter_532[i]\n",
    "    \n",
    "    print(f\"{i:<8} {x:<15.6f} {y:<15.6f} {z:<12.3f} {intensity:<12} {bs532:<12.6f}\")\n",
    "\n",
    "print(f\"\\nScale/Offset transformation:\")\n",
    "print(f\"  X: value * {las.header.scales[0]:.6f} + {las.header.offsets[0]:.6f}\")\n",
    "print(f\"  Y: value * {las.header.scales[1]:.6f} + {las.header.offsets[1]:.6f}\")\n",
    "print(f\"  Z: value * {las.header.scales[2]:.6f} + {las.header.offsets[2]:.6f}\")\n",
    "\n",
    "print(f\"\\nIntensity to Backscatter conversion:\")\n",
    "print(f\"  backscatter_532 = (intensity / 10000.0) - 0.1\")\n",
    "print(f\"  Example: {las.intensity[0]} â†’ {(las.intensity[0] / 10000.0) - 0.1:.6f}\")\n",
    "print(f\"  Actual:  {las.intensity[0]} â†’ {las.backscatter_532[0]:.6f} (from extra dim)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize point distribution by depth\n",
    "\n",
    "if copc_info and 'hierarchy' in globals():\n",
    "    # Count points at each depth\n",
    "    depth_stats = {}\n",
    "    for key, info in hierarchy.items():\n",
    "        if key.depth not in depth_stats:\n",
    "            depth_stats[key.depth] = {'nodes': 0, 'points': 0, 'bytes': 0}\n",
    "        depth_stats[key.depth]['nodes'] += 1\n",
    "        depth_stats[key.depth]['points'] += info['point_count']\n",
    "        depth_stats[key.depth]['bytes'] += info['byte_size']\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    depths = sorted(depth_stats.keys())\n",
    "    nodes = [depth_stats[d]['nodes'] for d in depths]\n",
    "    points = [depth_stats[d]['points'] for d in depths]\n",
    "    sizes = [depth_stats[d]['bytes'] / (1024**2) for d in depths]  # MB\n",
    "    \n",
    "    # Plot 1: Nodes per depth\n",
    "    ax1.bar(depths, nodes, color='steelblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Depth', fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Nodes', fontweight='bold')\n",
    "    ax1.set_title('Octree Nodes by Depth', fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Points per depth\n",
    "    ax2.bar(depths, points, color='coral', edgecolor='black')\n",
    "    ax2.set_xlabel('Depth', fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Points', fontweight='bold')\n",
    "    ax2.set_title('Points Distribution by Depth', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.ticklabel_format(style='plain', axis='y')\n",
    "    \n",
    "    # Plot 3: Data size per depth\n",
    "    ax3.bar(depths, sizes, color='seagreen', edgecolor='black')\n",
    "    ax3.set_xlabel('Depth', fontweight='bold')\n",
    "    ax3.set_ylabel('Data Size (MB)', fontweight='bold')\n",
    "    ax3.set_title('Compressed Data Size by Depth', fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nOctree Distribution Summary:\")\n",
    "    print(f\"{'Depth':<8} {'Nodes':<12} {'Points':<15} {'Avg Pts/Node':<15} {'Size (MB)':<12}\")\n",
    "    print(\"-\" * 75)\n",
    "    for d in depths:\n",
    "        avg_points = depth_stats[d]['points'] / depth_stats[d]['nodes']\n",
    "        print(f\"{d:<8} {depth_stats[d]['nodes']:<12,} {depth_stats[d]['points']:<15,} \"\n",
    "              f\"{avg_points:<15,.1f} {sizes[d]:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Optimization Strategies for Visualization\n",
    "\n",
    "### Progressive Loading (LOD)\n",
    "\n",
    "Load data in order of importance:\n",
    "\n",
    "1. **Load root** (depth 0) - Shows full extent immediately\n",
    "2. **Load depth 1** - 8 coarse regions  \n",
    "3. **Refine visible areas** - Based on camera position\n",
    "4. **Continue to target depth** - Until quality threshold met\n",
    "\n",
    "```python\n",
    "def progressive_load(hierarchy, camera_frustum, max_depth):\n",
    "    # Priority queue: (priority, node)\n",
    "    queue = [(0, root_node)]\n",
    "    loaded = set()\n",
    "    \n",
    "    while queue and len(loaded) < point_budget:\n",
    "        priority, node = heapq.heappop(queue)\n",
    "        \n",
    "        if node in loaded:\n",
    "            continue\n",
    "        \n",
    "        # Load this node\n",
    "        load_node(node)\n",
    "        loaded.add(node)\n",
    "        \n",
    "        # Add children if:\n",
    "        # 1. Not at max depth\n",
    "        # 2. Node is visible\n",
    "        # 3. Node is close enough to camera\n",
    "        if node.depth < max_depth:\n",
    "            for child in node.get_children():\n",
    "                if is_visible(child, camera_frustum):\n",
    "                    distance = distance_to_camera(child)\n",
    "                    heapq.heappush(queue, (distance, child))\n",
    "```\n",
    "\n",
    "### Frustum Culling\n",
    "\n",
    "Only load nodes visible in the camera view:\n",
    "\n",
    "```python\n",
    "def is_in_frustum(node_bbox, frustum_planes):\n",
    "    \"\"\"Test if bounding box intersects camera frustum.\"\"\"\n",
    "    for plane in frustum_planes:\n",
    "        # Check if bbox is completely outside this plane\n",
    "        if all_corners_outside(node_bbox, plane):\n",
    "            return False\n",
    "    return True\n",
    "```\n",
    "\n",
    "### Point Budget Management\n",
    "\n",
    "Limit total points rendered:\n",
    "\n",
    "```javascript\n",
    "const POINT_BUDGET = 5_000_000;  // 5M points max\n",
    "\n",
    "function selectNodes(hierarchy, camera) {\n",
    "    let nodes = [];\n",
    "    let totalPoints = 0;\n",
    "    \n",
    "    // Start with root, expand by priority\n",
    "    for (const node of priorityQueue) {\n",
    "        if (totalPoints + node.pointCount > POINT_BUDGET) {\n",
    "            break;  // Budget exceeded\n",
    "        }\n",
    "        nodes.push(node);\n",
    "        totalPoints += node.pointCount;\n",
    "    }\n",
    "    \n",
    "    return nodes;\n",
    "}\n",
    "```\n",
    "\n",
    "### Decimation Strategies\n",
    "\n",
    "For very dense nodes, skip points:\n",
    "\n",
    "```python\n",
    "def decimate_points(points, target_count):\n",
    "    if len(points) <= target_count:\n",
    "        return points\n",
    "    \n",
    "    # Option 1: Random sampling\n",
    "    indices = np.random.choice(len(points), target_count, replace=False)\n",
    "    return points[indices]\n",
    "    \n",
    "    # Option 2: Regular grid sampling\n",
    "    step = len(points) // target_count\n",
    "    return points[::step]\n",
    "    \n",
    "    # Option 3: Spatial sampling (preserve features)\n",
    "    # ... more complex algorithm ...\n",
    "```\n",
    "\n",
    "### Shader-Based Rendering\n",
    "\n",
    "Use GPU for efficient point rendering:\n",
    "\n",
    "```glsl\n",
    "// Vertex Shader\n",
    "attribute vec3 position;\n",
    "attribute float intensity;\n",
    "attribute float backscatter_532;\n",
    "\n",
    "uniform mat4 projectionMatrix;\n",
    "uniform mat4 viewMatrix;\n",
    "uniform float pointSize;\n",
    "\n",
    "varying vec3 vColor;\n",
    "\n",
    "void main() {\n",
    "    // Color by backscatter (log scale)\n",
    "    float value = log(backscatter_532 + 0.0001) / log(0.01);\n",
    "    vColor = viridis(clamp(value, 0.0, 1.0));\n",
    "    \n",
    "    // Screen-space point size (closer = larger)\n",
    "    vec4 mvPosition = viewMatrix * vec4(position, 1.0);\n",
    "    gl_PointSize = pointSize * (300.0 / -mvPosition.z);\n",
    "    \n",
    "    gl_Position = projectionMatrix * mvPosition;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate LOD selection algorithm\n",
    "\n",
    "def select_lod_nodes(hierarchy, camera_pos, camera_dir, max_depth=3, point_budget=1_000_000):\n",
    "    \"\"\"\n",
    "    Simulate LOD node selection based on camera position.\n",
    "    Simpler version without full frustum culling.\n",
    "    \"\"\"\n",
    "    selected = []\n",
    "    total_points = 0\n",
    "    \n",
    "    # Start with root\n",
    "    root = VoxelKey(0, 0, 0, 0)\n",
    "    if root not in hierarchy:\n",
    "        return selected\n",
    "    \n",
    "    # Priority queue: (negative priority, depth, node)\n",
    "    # Higher priority = process first\n",
    "    import heapq\n",
    "    queue = [(-100, 0, root)]\n",
    "    visited = set()\n",
    "    \n",
    "    while queue and total_points < point_budget:\n",
    "        priority, depth, node = heapq.heappop(queue)\n",
    "        \n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        \n",
    "        # Get node info\n",
    "        if node not in hierarchy:\n",
    "            continue\n",
    "        \n",
    "        info = hierarchy[node]\n",
    "        \n",
    "        # Check point budget\n",
    "        if total_points + info['point_count'] > point_budget:\n",
    "            continue\n",
    "        \n",
    "        # Add this node\n",
    "        selected.append((node, info))\n",
    "        total_points += info['point_count']\n",
    "        \n",
    "        # Add children if not at max depth\n",
    "        if node.depth < max_depth:\n",
    "            bounds = node.get_bounds(copc_info['center'], copc_info['halfsize'])\n",
    "            center = [(bounds['min'][i] + bounds['max'][i]) / 2 for i in range(3)]\n",
    "            \n",
    "            # Calculate distance to camera (simplified)\n",
    "            dist = np.sqrt(sum((center[i] - camera_pos[i])**2 for i in range(3)))\n",
    "            \n",
    "            for child in node.get_children():\n",
    "                if child in hierarchy:\n",
    "                    # Priority = inverse distance (closer = higher priority)\n",
    "                    child_priority = -dist\n",
    "                    heapq.heappush(queue, (child_priority, child.depth, child))\n",
    "    \n",
    "    return selected, total_points\n",
    "\n",
    "if copc_info and 'hierarchy' in globals():\n",
    "    # Simulate from different camera positions\n",
    "    test_cameras = [\n",
    "        {\"pos\": [0, 0, 20], \"desc\": \"Overhead view\"},\n",
    "        {\"pos\": [0, 50, 10], \"desc\": \"Mid-latitude view\"},\n",
    "        {\"pos\": [100, 0, 10], \"desc\": \"Side view\"},\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LOD SELECTION SIMULATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nPoint budget: 1,000,000 points\")\n",
    "    print(f\"Max depth: 3\\n\")\n",
    "    \n",
    "    for cam in test_cameras:\n",
    "        selected, total = select_lod_nodes(\n",
    "            hierarchy, \n",
    "            cam[\"pos\"], \n",
    "            [0, 0, -1],  # Looking down\n",
    "            max_depth=3,\n",
    "            point_budget=1_000_000\n",
    "        )\n",
    "        \n",
    "        depth_counts = {}\n",
    "        for node, info in selected:\n",
    "            depth_counts[node.depth] = depth_counts.get(node.depth, 0) + 1\n",
    "        \n",
    "        print(f\"{cam['desc']}:\")\n",
    "        print(f\"  Camera at: {cam['pos']}\")\n",
    "        print(f\"  Selected: {len(selected)} nodes, {total:,} points\")\n",
    "        print(f\"  Distribution: \", end=\"\")\n",
    "        for d in sorted(depth_counts.keys()):\n",
    "            print(f\"L{d}={depth_counts[d]} \", end=\"\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Common Issues & Solutions\n",
    "\n",
    "### Issue 1: Memory Management\n",
    "\n",
    "**Problem:** Loading too many points crashes browser/application\n",
    "\n",
    "**Solutions:**\n",
    "```javascript\n",
    "// 1. Implement strict point budget\n",
    "const MAX_POINTS = 10_000_000;\n",
    "\n",
    "// 2. Unload distant nodes\n",
    "function unloadDistantNodes(nodes, camera, threshold) {\n",
    "    return nodes.filter(node => {\n",
    "        if (distance(node, camera) > threshold) {\n",
    "            node.unload();  // Free GPU/CPU memory\n",
    "            return false;\n",
    "        }\n",
    "        return true;\n",
    "    });\n",
    "}\n",
    "\n",
    "// 3. Use typed arrays for efficiency\n",
    "const positions = new Float32Array(pointCount * 3);\n",
    "const colors = new Uint8Array(pointCount * 3);\n",
    "```\n",
    "\n",
    "### Issue 2: Network Latency\n",
    "\n",
    "**Problem:** Slow loading of individual chunks\n",
    "\n",
    "**Solutions:**\n",
    "```javascript\n",
    "// 1. Prefetch nearby nodes\n",
    "function prefetchNeighbors(currentNode) {\n",
    "    const neighbors = getAdjacentNodes(currentNode);\n",
    "    neighbors.forEach(node => {\n",
    "        if (!cache.has(node)) {\n",
    "            fetchNodeAsync(node);  // Background fetch\n",
    "        }\n",
    "    });\n",
    "}\n",
    "\n",
    "// 2. Use HTTP/2 multiplexing\n",
    "// Multiple requests on single connection\n",
    "\n",
    "// 3. Compress hierarchy (if large)\n",
    "// Store hierarchy in separate gzipped JSON\n",
    "```\n",
    "\n",
    "### Issue 3: Coordinate System Issues\n",
    "\n",
    "**Problem:** Points appear in wrong location or distorted\n",
    "\n",
    "**Solutions:**\n",
    "```python\n",
    "# 1. Verify CRS matches expectations\n",
    "crs = las.header.parse_crs()\n",
    "print(f\"CRS: {crs}\")  # Should be EPSG:4326 for geographic\n",
    "\n",
    "# 2. Check scale/offset values\n",
    "# For geographic coords (degrees), scale should be ~1e-6\n",
    "# For projected coords (meters), scale should be ~0.01\n",
    "\n",
    "# 3. Transform if needed\n",
    "from pyproj import Transformer\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\")\n",
    "x_web, y_web = transformer.transform(lat, lon)\n",
    "```\n",
    "\n",
    "### Issue 4: LAZ Decompression\n",
    "\n",
    "**Problem:** Cannot read compressed chunks in browser\n",
    "\n",
    "**Solutions:**\n",
    "```javascript\n",
    "// 1. Use las-js library\n",
    "import { LASFile } from 'las-js';\n",
    "\n",
    "async function readCompressedChunk(offset, size) {\n",
    "    const data = await fetchRange(offset, size);\n",
    "    const lasFile = new LASFile(data);\n",
    "    return lasFile.getPoints();\n",
    "}\n",
    "\n",
    "// 2. Use WebAssembly LAZ decoder\n",
    "import lazPerf from 'laz-perf';\n",
    "\n",
    "// 3. Pre-decompress on server if needed\n",
    "```\n",
    "\n",
    "### Issue 5: Browser Limitations\n",
    "\n",
    "**Problem:** WebGL texture/buffer size limits\n",
    "\n",
    "**Solutions:**\n",
    "```javascript\n",
    "// 1. Check limits\n",
    "const gl = canvas.getContext('webgl2');\n",
    "const maxTextureSize = gl.getParameter(gl.MAX_TEXTURE_SIZE);\n",
    "const maxArraySize = gl.getParameter(gl.MAX_ARRAY_TEXTURE_LAYERS);\n",
    "\n",
    "// 2. Split large datasets\n",
    "function splitIntoBuffers(points, maxSize) {\n",
    "    const buffers = [];\n",
    "    for (let i = 0; i < points.length; i += maxSize) {\n",
    "        buffers.push(points.slice(i, i + maxSize));\n",
    "    }\n",
    "    return buffers;\n",
    "}\n",
    "\n",
    "// 3. Use instancing for repeated geometry\n",
    "```\n",
    "\n",
    "### Issue 6: Color Mapping\n",
    "\n",
    "**Problem:** Backscatter values don't visualize well\n",
    "\n",
    "**Solutions:**\n",
    "```python\n",
    "# 1. Use log scale for backscatter\n",
    "value_normalized = (np.log10(bs + 1e-5) + 5) / 3  # Maps 1e-5 to 0.01\n",
    "\n",
    "# 2. Apply percentile stretching\n",
    "p2, p98 = np.percentile(backscatter, [2, 98])\n",
    "value_stretched = np.clip((bs - p2) / (p98 - p2), 0, 1)\n",
    "\n",
    "# 3. Use good colormaps\n",
    "# - viridis: general purpose\n",
    "# - turbo: high dynamic range\n",
    "# - coolwarm: diverging data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate color mapping strategies\n",
    "\n",
    "# Sample backscatter data\n",
    "bs_sample = las.backscatter_532[::1000]  # Every 1000th point\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Strategy 1: Linear scale\n",
    "ax = axes[0, 0]\n",
    "linear = np.clip(bs_sample / 0.01, 0, 1)\n",
    "ax.hist(linear, bins=50, color='steelblue', edgecolor='black')\n",
    "ax.set_title('Linear Scale\\n(Poor: most values near 0)', fontweight='bold')\n",
    "ax.set_xlabel('Normalized Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "# Strategy 2: Log scale\n",
    "ax = axes[0, 1]\n",
    "log_scale = (np.log10(bs_sample + 1e-5) + 5) / 3\n",
    "log_scale = np.clip(log_scale, 0, 1)\n",
    "ax.hist(log_scale, bins=50, color='coral', edgecolor='black')\n",
    "ax.set_title('Log Scale\\n(Better: spreads out values)', fontweight='bold')\n",
    "ax.set_xlabel('Normalized Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "# Strategy 3: Percentile stretch\n",
    "ax = axes[0, 2]\n",
    "p2, p98 = np.percentile(bs_sample, [2, 98])\n",
    "percentile = np.clip((bs_sample - p2) / (p98 - p2), 0, 1)\n",
    "ax.hist(percentile, bins=50, color='seagreen', edgecolor='black')\n",
    "ax.set_title('Percentile Stretch (2-98%)\\n(Best: uses full range)', fontweight='bold')\n",
    "ax.set_xlabel('Normalized Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "# Show colormaps\n",
    "cmaps = ['viridis', 'plasma', 'turbo']\n",
    "for i, cmap_name in enumerate(cmaps):\n",
    "    ax = axes[1, i]\n",
    "    gradient = np.linspace(0, 1, 256).reshape(1, -1)\n",
    "    ax.imshow(gradient, aspect='auto', cmap=cmap_name)\n",
    "    ax.set_title(f'{cmap_name.capitalize()} Colormap', fontweight='bold')\n",
    "    ax.set_xticks([0, 128, 256])\n",
    "    ax.set_xticklabels(['Low', 'Medium', 'High'])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Color Mapping Recommendations:\")\n",
    "print(\"  1. Log scale: Best for backscatter (spans many orders of magnitude)\")\n",
    "print(\"  2. Percentile stretch: Good for automatic contrast\")\n",
    "print(\"  3. Viridis colormap: Perceptually uniform, colorblind-friendly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Practical Example - Building a Simple Viewer\n",
    "\n",
    "### Minimal COPC Viewer Architecture\n",
    "\n",
    "```javascript\n",
    "class COPCViewer {\n",
    "    constructor(filename) {\n",
    "        this.filename = filename;\n",
    "        this.hierarchy = null;\n",
    "        this.loadedNodes = new Map();\n",
    "        this.pointBudget = 5_000_000;\n",
    "    }\n",
    "    \n",
    "    async init() {\n",
    "        // 1. Load header and COPC info\n",
    "        this.copcInfo = await this.loadCOPCInfo();\n",
    "        \n",
    "        // 2. Load hierarchy\n",
    "        this.hierarchy = await this.loadHierarchy();\n",
    "        \n",
    "        // 3. Load root node\n",
    "        await this.loadNode(new VoxelKey(0, 0, 0, 0));\n",
    "    }\n",
    "    \n",
    "    async loadNode(key) {\n",
    "        const entry = this.hierarchy.get(key);\n",
    "        if (!entry) return;\n",
    "        \n",
    "        // Fetch chunk via HTTP range\n",
    "        const data = await this.fetchRange(\n",
    "            entry.offset,\n",
    "            entry.byteSize\n",
    "        );\n",
    "        \n",
    "        // Decompress if needed\n",
    "        const points = await this.decompressLAZ(data);\n",
    "        \n",
    "        // Store in GPU buffer\n",
    "        this.loadedNodes.set(key, points);\n",
    "    }\n",
    "    \n",
    "    updateView(camera) {\n",
    "        // Select nodes based on camera\n",
    "        const visible = this.selectVisibleNodes(camera);\n",
    "        \n",
    "        // Load missing nodes\n",
    "        for (const node of visible) {\n",
    "            if (!this.loadedNodes.has(node)) {\n",
    "                this.loadNode(node);\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Unload distant nodes\n",
    "        this.unloadDistantNodes(camera);\n",
    "        \n",
    "        // Render\n",
    "        this.render();\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Integration with deck.gl\n",
    "\n",
    "```javascript\n",
    "import { Deck } from '@deck.gl/core';\n",
    "import { PointCloudLayer } from '@deck.gl/layers';\n",
    "\n",
    "class DeckGLCOPCLayer extends PointCloudLayer {\n",
    "    constructor(props) {\n",
    "        super(props);\n",
    "        this.copcLoader = new COPCViewer(props.url);\n",
    "    }\n",
    "    \n",
    "    async initialize() {\n",
    "        await this.copcLoader.init();\n",
    "        this.setNeedsRedraw();\n",
    "    }\n",
    "    \n",
    "    updateState({ changeFlags, props, context }) {\n",
    "        if (changeFlags.viewportChanged) {\n",
    "            const { viewport } = context;\n",
    "            this.copcLoader.updateView(viewport.camera);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    renderLayers() {\n",
    "        const points = Array.from(this.copcLoader.loadedNodes.values()).flat();\n",
    "        \n",
    "        return new PointCloudLayer({\n",
    "            id: 'copc-points',\n",
    "            data: points,\n",
    "            getPosition: d => [d.x, d.y, d.z],\n",
    "            getColor: d => this.colorByBackscatter(d.backscatter_532),\n",
    "            pointSize: 2\n",
    "        });\n",
    "    }\n",
    "    \n",
    "    colorByBackscatter(value) {\n",
    "        // Log scale to color\n",
    "        const normalized = (Math.log10(value + 1e-5) + 5) / 3;\n",
    "        return viridis(Math.max(0, Math.min(1, normalized)));\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmark: Node loading simulation\n",
    "\n",
    "import time\n",
    "\n",
    "def benchmark_node_access(hierarchy, num_nodes=100):\n",
    "    \"\"\"\n",
    "    Simulate random node access patterns.\n",
    "    Measures overhead of file seeks.\n",
    "    \"\"\"\n",
    "    # Get random nodes\n",
    "    nodes = list(hierarchy.keys())[:num_nodes]\n",
    "    \n",
    "    times = []\n",
    "    \n",
    "    with open(COPC_FILE, 'rb') as f:\n",
    "        for node in nodes:\n",
    "            info = hierarchy[node]\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            # Seek to offset\n",
    "            f.seek(info['offset'])\n",
    "            \n",
    "            # Read data (but don't decompress)\n",
    "            data = f.read(info['byte_size'])\n",
    "            \n",
    "            elapsed = time.time() - start\n",
    "            times.append(elapsed)\n",
    "    \n",
    "    return times\n",
    "\n",
    "if 'hierarchy' in globals():\n",
    "    print(\"=\"*80)\n",
    "    print(\"PERFORMANCE BENCHMARK\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nSimulating random node access...\")\n",
    "    \n",
    "    times = benchmark_node_access(hierarchy, num_nodes=50)\n",
    "    \n",
    "    print(f\"\\nResults for 50 random nodes:\")\n",
    "    print(f\"  Mean time:   {np.mean(times)*1000:.2f} ms\")\n",
    "    print(f\"  Median time: {np.median(times)*1000:.2f} ms\")\n",
    "    print(f\"  Min time:    {np.min(times)*1000:.2f} ms\")\n",
    "    print(f\"  Max time:    {np.max(times)*1000:.2f} ms\")\n",
    "    \n",
    "    print(f\"\\nProjected throughput:\")\n",
    "    nodes_per_sec = 1.0 / np.mean(times)\n",
    "    avg_points_per_node = sum(hierarchy[k]['point_count'] for k in hierarchy) / len(hierarchy)\n",
    "    points_per_sec = nodes_per_sec * avg_points_per_node\n",
    "    \n",
    "    print(f\"  Nodes/sec:  {nodes_per_sec:.0f}\")\n",
    "    print(f\"  Points/sec: {points_per_sec:,.0f}\")\n",
    "    print(f\"\\nğŸ’¡ For 60 FPS, you can load ~{nodes_per_sec/60:.0f} nodes per frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Advanced Topics\n",
    "\n",
    "### Spatial Queries\n",
    "\n",
    "Find nodes intersecting a region:\n",
    "\n",
    "```python\n",
    "def query_region(hierarchy, bbox, copc_info):\n",
    "    \"\"\"\n",
    "    Find all nodes that intersect the bounding box.\n",
    "    bbox: [[min_x, max_x], [min_y, max_y], [min_z, max_z]]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    def intersects(node_bounds, query_bbox):\n",
    "        for i in range(3):\n",
    "            if (node_bounds['max'][i] < query_bbox[i][0] or \n",
    "                node_bounds['min'][i] > query_bbox[i][1]):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    for key, info in hierarchy.items():\n",
    "        bounds = key.get_bounds(copc_info['center'], copc_info['halfsize'])\n",
    "        if intersects(bounds, bbox):\n",
    "            result.append((key, info))\n",
    "    \n",
    "    return result\n",
    "```\n",
    "\n",
    "### Custom Point Cloud Format\n",
    "\n",
    "For specialized applications, create optimized formats:\n",
    "\n",
    "```python\n",
    "# Export to binary format optimized for GPU\n",
    "def export_gpu_friendly(copc_file, output_file):\n",
    "    las = laspy.read(copc_file)\n",
    "    \n",
    "    # Pack data tightly\n",
    "    data = np.zeros(len(las.points), dtype=[\n",
    "        ('x', 'f4'),      # 4 bytes\n",
    "        ('y', 'f4'),      # 4 bytes  \n",
    "        ('z', 'f4'),      # 4 bytes\n",
    "        ('bs', 'f2'),     # 2 bytes (half precision)\n",
    "        ('class', 'u1'),  # 1 byte\n",
    "        # Total: 15 bytes per point\n",
    "    ])\n",
    "    \n",
    "    data['x'] = las.x.astype(np.float32)\n",
    "    data['y'] = las.y.astype(np.float32)\n",
    "    data['z'] = las.z.astype(np.float32)\n",
    "    data['bs'] = las.backscatter_532.astype(np.float16)\n",
    "    data['class'] = las.classification\n",
    "    \n",
    "    data.tofile(output_file)\n",
    "```\n",
    "\n",
    "### Streaming Architecture\n",
    "\n",
    "For production systems:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Client  â”‚\n",
    "â”‚(Browser)â”‚\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "     â”‚ HTTP/2\n",
    "     â”‚ Range: bytes=1000-2000\n",
    "     â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   CDN       â”‚  (CloudFront, Fastly)\n",
    "â”‚   Cache     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "      â”‚\n",
    "      â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   S3/GCS    â”‚  (Object storage)\n",
    "â”‚  COPC files â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Best Practices Summary\n",
    "\n",
    "1. **Always** use HTTP/2 for multiplexing\n",
    "2. **Cache** hierarchy in memory/localStorage\n",
    "3. **Prefetch** likely nodes (neighbors, next LOD)\n",
    "4. **Unload** distant nodes to save memory\n",
    "5. **Use WebWorkers** for decompression\n",
    "6. **Implement** frustum culling\n",
    "7. **Monitor** performance with metrics\n",
    "8. **Test** on mobile devices (memory limits)\n",
    "9. **Provide** LOD controls to users\n",
    "10. **Log** telemetry for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize octree structure in 3D\n",
    "\n",
    "if copc_info and 'hierarchy' in globals():\n",
    "    # Select first few levels for visualization\n",
    "    nodes_to_viz = [k for k in hierarchy.keys() if k.depth <= 2]\n",
    "    \n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    for node in nodes_to_viz:\n",
    "        bounds = node.get_bounds(copc_info['center'], copc_info['halfsize'])\n",
    "        \n",
    "        # Draw bounding box\n",
    "        # Get 8 corners\n",
    "        corners = []\n",
    "        for i in [0, 1]:\n",
    "            for j in [0, 1]:\n",
    "                for k in [0, 1]:\n",
    "                    x = bounds['min'][0] if i == 0 else bounds['max'][0]\n",
    "                    y = bounds['min'][1] if j == 0 else bounds['max'][1]\n",
    "                    z = bounds['min'][2] if k == 0 else bounds['max'][2]\n",
    "                    corners.append([x, y, z])\n",
    "        \n",
    "        corners = np.array(corners)\n",
    "        \n",
    "        # Draw edges\n",
    "        edges = [\n",
    "            [0, 1], [0, 2], [0, 4],  # From (0,0,0)\n",
    "            [1, 3], [1, 5],          # From (1,0,0)\n",
    "            [2, 3], [2, 6],          # From (0,1,0)\n",
    "            [3, 7],                  # From (1,1,0)\n",
    "            [4, 5], [4, 6],          # From (0,0,1)\n",
    "            [5, 7], [6, 7]           # To (1,1,1)\n",
    "        ]\n",
    "        \n",
    "        color = colors[node.depth % len(colors)]\n",
    "        alpha = 0.3 if node.depth > 0 else 0.8\n",
    "        \n",
    "        for edge in edges:\n",
    "            ax.plot3D(\n",
    "                [corners[edge[0]][0], corners[edge[1]][0]],\n",
    "                [corners[edge[0]][1], corners[edge[1]][1]],\n",
    "                [corners[edge[0]][2], corners[edge[1]][2]],\n",
    "                color=color, alpha=alpha, linewidth=1\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel('Longitude (Â°)', fontweight='bold')\n",
    "    ax.set_ylabel('Latitude (Â°)', fontweight='bold')\n",
    "    ax.set_zlabel('Altitude (km)', fontweight='bold')\n",
    "    ax.set_title('COPC Octree Structure (Levels 0-2)', fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [Line2D([0], [0], color=colors[i], lw=2, label=f'Level {i}') \n",
    "                      for i in range(min(3, max_depth + 1))]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Visualized {len(nodes_to_viz)} nodes (levels 0-2)\")\n",
    "    print(f\"Red = Level 0 (root), Blue = Level 1, Green = Level 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Summary & Resources\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **COPC = Streamable Point Clouds**\n",
    "   - Octree organization enables partial reads\n",
    "   - HTTP range requests for efficient streaming\n",
    "   - LOD built into file structure\n",
    "\n",
    "2. **Understanding the Format**\n",
    "   - VLRs contain metadata (COPC Info)\n",
    "   - EVLRs contain hierarchy (node index)\n",
    "   - Point data organized as octree chunks\n",
    "   - Each chunk is independently compressed\n",
    "\n",
    "3. **Optimization is Essential**\n",
    "   - Point budget management\n",
    "   - Progressive LOD loading\n",
    "   - Frustum culling\n",
    "   - Memory management (unload distant nodes)\n",
    "   - GPU-based rendering\n",
    "\n",
    "4. **Common Patterns**\n",
    "   - Load root first (immediate context)\n",
    "   - Refine based on camera position\n",
    "   - Prefetch likely nodes\n",
    "   - Cache hierarchy\n",
    "   - Use appropriate color scales\n",
    "\n",
    "### For CALIPSO Data Specifically\n",
    "\n",
    "- **Sparse 3D structure** - Vertical curtains, not dense volumes\n",
    "- **Two wavelengths** - Use color or size to show 532/1064\n",
    "- **Large datasets** - 300+ MB requires streaming\n",
    "- **Scientific values** - Extra dimensions preserve float64 precision\n",
    "- **Log scale essential** - Backscatter spans many orders of magnitude\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Specifications:**\n",
    "- [COPC Specification](https://copc.io/)\n",
    "- [LAS 1.4 Specification](https://www.asprs.org/divisions-committees/lidar-division/laser-las-file-format-exchange-activities)\n",
    "- [LAZ Compression](https://laszip.org/)\n",
    "\n",
    "**Tools & Libraries:**\n",
    "- **Python**: `laspy`, `pdal`, `pyproj`\n",
    "- **JavaScript**: `las-js`, `copc.js`, `potree`, `deck.gl`\n",
    "- **C++**: `PDAL`, `LAStools`, `entwine`\n",
    "\n",
    "**Viewers:**\n",
    "- [Potree](http://potree.org/) - WebGL point cloud viewer\n",
    "- [Plasio](https://github.com/hobu/plasio) - LAZ streaming viewer\n",
    "- [QGIS](https://qgis.org/) - Desktop GIS with point cloud support\n",
    "\n",
    "**Services:**\n",
    "- [Cesium Ion](https://cesium.com/platform/cesium-ion/) - Point cloud tiling\n",
    "- [Entwine](https://entwine.io/) - Point cloud organization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Implement a viewer** - Start with deck.gl or Three.js\n",
    "2. **Optimize for your use case** - Profile and measure\n",
    "3. **Add interactivity** - Picking, measurements, filters\n",
    "4. **Deploy to cloud** - S3 + CloudFront\n",
    "5. **Monitor performance** - Real-user metrics\n",
    "\n",
    "### Example Projects\n",
    "\n",
    "- **CALIPSO Browser**: Interactive satellite lidar visualization\n",
    "- **Cloud Classifier**: ML-based cloud type identification\n",
    "- **Vertical Profiler**: Extract atmospheric profiles at locations\n",
    "- **Time Series Viewer**: Compare multiple CALIPSO passes\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You now understand:\n",
    "- âœ… COPC file structure and VLRs\n",
    "- âœ… Octree organization and LOD\n",
    "- âœ… Byte offsets and chunk reading\n",
    "- âœ… Point data encoding and decoding\n",
    "- âœ… Optimization strategies for visualization\n",
    "- âœ… Common issues and solutions\n",
    "- âœ… Advanced topics and best practices\n",
    "\n",
    "**Ready to build amazing point cloud visualizations!** ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}